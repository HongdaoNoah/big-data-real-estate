version: "3.8"

services:
  spark:
    image: apache/spark:3.5.0
    container_name: spark
    working_dir: /opt/project
    volumes:
      - ../:/opt/project
    command: ["bash", "-lc", "sleep infinity"]

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
    ports:
      - "9200:9200"
    volumes:
      - es_data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.12.0
    container_name: kibana
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch


    airflow:
    image: apache/airflow:2.8.1
    container_name: airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ../dags:/opt/airflow/dags
      - ../scripts:/opt/airflow/scripts
      - ../data:/opt/airflow/data
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command: >
      bash -c "
      airflow db init &&
      airflow users create
        --username admin
        --password admin
        --firstname admin
        --lastname admin
        --role Admin
        --email admin@airflow.local &&
      airflow scheduler &
      airflow webserver
      "


volumes:
  es_data:


